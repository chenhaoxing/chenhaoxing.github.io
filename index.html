<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Haoxing Chen's Homepage</title>
  <link href="css/bootstrap.css" rel='stylesheet' type='text/css' />
  <link rel="shortcut icon" type="image/x-icon" href="./Files/mambaicon.png">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <!-- Bulma Version 0.7.5-->
  <link rel="stylesheet" href="https://unpkg.com/bulma@0.7.5/css/bulma.min.css" />
  <link href='css/style.css?_t=20200916' rel='stylesheet' type='text/css'>
  <script defer src="font-awesome/js/brands.min.js"></script>
  <script defer src="font-awesome/js/regular.min.js"></script>
  <script defer src="font-awesome/js/fontawesome.min.js"></script>
  <style>
    .blue {
      color: rgb(238, 77, 44);
      font-style: normal.
    }

    #intro {
      margin-top: 0em !important;
    }

    .content h3 {
      margin-bottom: 1em!important;
      margin-top: 2em!important;
    }

    .columns:not(:last-child) {
      margin-bottom: 1.75rem!important;
    }

    #sidebar {
      width: 75%;
    }

    .center-content {
      display: flex;
      align-items: center;
    }

	  .button {
    font-family: Georgia, serif;
    background-color: #D6EAF8;
    border: none;
    color: #022b6d;
    padding: 6px 12px;
    text-decoration: none;
    display: inline-block;
    font-size: 0.95em;
    margin: 2px 3px;
    border-radius: 4px;
    transition: all 0.3s ease;
  }
  
  .button:hover {
    background-color: #AED6F1;
    transform: translateY(-2px);
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
  }
  
  .button3 {
    background-color: #F1948A;
    color: white;
  }
  
  .button3:hover {
    background-color: #E74C3C;
  }
  
  .button4 {
    background-color: #D6EAF8;
    font-size: 0.85em;
  }
  
  .button4:hover {
    background-color: #AED6F1;
  }
  
  .button5 {
    background-color: #5DADE2;
    color: white;
  }
  
  .button5:hover {
    background-color: #3498DB;
  }
  /* Further compact buttons */
  .button {
    padding: 4px 8px;
    margin: 0 2px;
    line-height: 1.2;
  }
  
  /* Tighten buttons row */
  #publications ol li button, #preprints ol li button {
    margin-top: 2px;
  }
  /* Buttons should be on same line with minimal spacing */
  button.button {
    margin: 2px 2px;
    padding: 3px 7px;
    font-size: 0.9em;
  }
  /* Fix spacing before buttons */
  #publications ol li button.button, #preprints ol li button.button {
    margin-top: 0;
    padding-top: 0;
    padding-bottom: 0;
  }
  /* Conference rate button very compact */
  button.button.button3 {
    padding: 3px 6px;
    margin-left: 5px;
  }
  
  /* Ensure buttons align properly */
  #publications ol li .button4, #preprints ol li .button4 {
    display: inline-block;
    margin: 1px 1px;
  }
	  

	  
    @media screen and (min-width: 769px),
    print {
      .column.is-3_10,
      .column.is-2-tablet {
        flex: none;
        width: 17%;
      }
    }
  </style>
  <script src="https://code.jquery.com/jquery-3.1.1.min.js" crossorigin="anonymous"></script>
</head>

<body>
  <section class="section">
    <div class="container">
      <div class="columns">
        <div class="column is-3_10">
          <div class="sticky">
             <figure class="image small-image" style="margin-top: 6px;">
              <img src="./Files/chx.jpg">
            </figure>
		  
            <div class="content">
              <h2 style="margin-top: 1em">Haoxing Chen</h2>
              <p>
		DeepLearning Lab, <br/> Ant Research Institute,<br/> 
                Hangzhou, CHINA<br/> 
              </p>
		    <div class="slogan" style="font-style: italic; text-align: center; color: #4a4a4a;">
  "Everything should be made as simple as possible, but no simpler." — Albert Einstein
</div>
            </div>
            <!-- social network icons -->
            <div class="social">
              <a href="https://github.com/chenhaoxing" target="_blank">
                <span class="fab fa-github fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a;"></span>
              </a>
              <a href="https://scholar.google.co.uk/citations?user=BnS7HzAAAAAJ&hl=en" target="_blank">
                <span class="fab fa-google fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a; margin-left: 15px"></span>
              </a>
              <a href="mailto:hx.chen@hotmail.com" target="_blank">
                <span class="fa-regular fa-envelope-open fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a; margin-left: 15px"></span>
              </a>
            </div>
            <!-- slogon -->
            <div id="sidebar" class="menu sticky is-hidden-mobile">
              <p class="menu-label"><b>Quick Links</b></p>
              <ul class="menu-list">
                <li><a href="#intro">About Me</a></li>
                <li><a href="#interest">Research Interest</a></li>
		<li><a href="#experiences">Experiences</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#awards">Awards</a></li>
                <li><a href="#services">Services</a></li>
              </ul>
            </div>
	 </div>
        </div>

        <div class="column right-panel">
          <div class="content">
<!-- 	 <h2 id="hire">Hiring</h2>
		  <p>
	<strong><font color=#e74d3c>We are seeking self-motivated PhD students or outstanding master students for research internships focused on efficient learning for MLLM. Feel free to contact us. </font></strong>
		  </p> -->
		  
            <!--About Me-->
            <h2 id="intro">About Me</h2>
            <p>
               I am an Artificial Intelligence Research Fellow with DeepLearning Lab at <a href="https://www.antresearch.com/">Ant Research Institute</a>, which is under the leadership of CTO <a href="https://www.weforum.org/people/he-zhengyu/">Zhengyu He</a>. My research benefits from collaboration with esteemed colleagues including Dr. <a href="https://scholar.google.com/citations?user=n44GlFcAAAAJ&hl=zh-CN&oi=ao">Jianguo Li</a>, Dr. <a href="https://scholar.google.com/citations?user=pC2kmQoAAAAJ&hl=zh-CN">Yaohui Li</a>, Dr. <a href="https://scholar.google.com/citations?user=Wkp3s68AAAAJ&hl=zh-CN">Zhangxuan Gu</a>, Dr. <a href="https://scholar.google.com/citations?user=ztq5-xcAAAAJ&hl=zh-CN">Yan Hong</a>, and Scientist <a href="https://scholar.google.com/citations?user=na24qQoAAAAJ&hl=zh-CN">Zhuoer Xu</a>.
            </p>

            <p>
             I pursued my M.E. in Automation and Artificial Intelligence Group at Nanjing University (NJU), where I was mentored by Prof. <a href="https://scholar.google.com/citations?user=5kXEo74AAAAJ&hl=zh-CN">Chunlin Chen</a> and Prof. <a href="https://scholar.google.com/citations?user=AC-EDw0AAAAJ&hl=zh-CN">Huaxiong Li</a>. My B.E. was obtained at Southeast University (SEU). 
          My research interests include few-shot learning, image generation, self-supervised learning, computer vision, and machine learning. Currently, I focus on Representation Learning, AIGC, and Learning with Limited Data.
             </p>

	<!--News-->
	<h2 id="interest" style="margin-bottom: 25px;">News</h2>
	<div style="height: 250px; overflow: auto;">
	<ul>
	<li><strong>[May/2025]: </strong>One paper on “Vision mamba” is accepted to <a href="https://icml.cc/">ICML 2025</a>.</li>
	<li><strong>[Mar./2025]: </strong>I have joined the DeepLearning Lab at Ant Research Institute to pursue cutting-edge research in Artificial General Intelligence (AGI).</li>
	<li><strong>[Feb/2025]: </strong>One paper on “Multi-modal foundation models” is accepted to <a href="https://cvpr.thecvf.com/">CVPR 2025</a>.</li>
	<li><strong>[Dec/2024]: </strong>One paper on “AIGC detection” is accepted to <a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a> as Oral representation.</li>
	<li><strong>[July/2024]: </strong>One paper on “Multi-view clustering” is accepted to <a href="https://www.sciencedirect.com/journal/pattern-recognition">Pattern Recognition</a>.</li>
	<li><strong>[July/2024]: </strong>We showcased our achievements (HDNet/DiffUTE/DeMamba/etc.) in generation and detection at the <a href="https://mp.weixin.qq.com/s/W-S06dxda1hLSsF2RsVtrQ">World Artificial Intelligence Conference</a>.</li>
	<li><strong>[July/2024]: </strong>One paper on “AIGC” is accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a>.</li>
	<li><strong>[Dec./2023]: </strong>Two paper on “Diffusion model” and “Image composition” is accepted to <a href="https://2024.ieeeicassp.org/">ICASSP 2024</a>.</li>
	<li><strong>[Sep./2023]: </strong>One paper on “AIGC” is accepted to <a href="https://nips.cc/">NeurIPS 2023</a>.</li>
	<li><strong>[Aug/2023]: </strong>We won 2nd place (2/717) in the tamper-proof financial documents track in the <a href="https://tianchi.aliyun.com/competition/entrance/532096/information">AFAC Financial Data Verification Competition</a></li>
	<li><strong>[July/2023]: </strong>One paper on “Image composition” is accepted to <a href="https://www.acmmm2023.org/">ACM Multimedia 2023</a> as Oral representation.</li>
	<li><strong>[April/2023]: </strong>One paper on “Contrastive learning” is accepted to <a href="https://icml.cc/Conferences/2023/">ICML 2023</a>.</li>
	<li><strong>[Mar./2023]: </strong>We won 3rd place (3/1267) in the classification track and 6th place (6/1156) in the detection track in the <a href="https://tianchi.aliyun.com/competition/entrance/532048/introduction?spm=5176.12281976.0.0.4562331137uPvV">ICDAR Detecting Tampered Text in Images Competition</a>.</li>
	<li><strong>[Feb./2023]: </strong>One paper on “Vision-language learning” is accepted to <a href="https://cvpr.thecvf.com/">CVPR 2023</a>.</li>
	<li><strong>[Jan./2023]: </strong>One paper on “Few-shot learning” is accepted to <a href="http://engine.scichina.com/doi/10.1007/s11432-022-3700-8">SCIS 2023</a>.</li>
	<li><strong>[July/2022]: </strong>One paper on “Affective computing” is accepted to <a href="https://2022.acmmm.org/">ACM Multimedia 2022</a>.</li>
	</ul>
	</div>
	<br />
		  
	<!--Research Interest-->
            <h2 id="interest" style="margin-bottom: 25px;">Research Interest</h2>
	I work in the field of few-shot learning, image generation, self-supervised learning, computer vision and machine learning. Currently, I focus on the following research topics:
	<ul>
	<li><strong>Representation Learning:</strong> Representation learning aims to discover abstract descriptions of concepts. Specifically, Haoxing focuses on enhancing the universality of the model through self-supervised learning and multimodal learning.</li>
	<li><strong>AIGC:</strong> How to design better defense systems to deal with generated attacks has gained extensive attention in recent years. Specifically, Haoxing trys to generate more realistic images and design better detection methods with multi-modal learning.</li>
	<li><strong>Learning with Limited Data:</strong> The ability of a model to fit with limited data is essential and necessary due to the instance/label collection cost. How to extract and utilize knowledge from related tasks and domains is the key. Specifically, Haoxing mainly works on how to learn meta-knowledge for zero-/few-shot learning.</li>
	</ul>
	<br />
		  
        <!--Experience-->
            <h2 id="experiences" style="margin-bottom: 25px;">Experiences</h2>

	<article class="columns">
              <div class="column is-3" >
                <div class="image">
                  <img src="./Files/antresearch.png" alt="WSFG">
                </div>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>AI Researcher</b> | DeepLearning Lab, Ant Research Institute<br>
                 Mar 2025 - Present.</a>
                  </p>
                </div>
              </div>
	</article>

		  
            <article class="columns">
              <div class="column is-3" >
                <div class="image">
                  <img src="./Files/ant.png" alt="WSFG">
                </div>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>AI Researcher</b> | Tiansuan Lab, Ant Group<br>
                 May 2022 - Mar 2025</a>
                  </p>
                </div>
              </div>
	</article>

	<article class="columns">
              <div class="column is-3" >
                <div class="image">
                  <img src="./Files/nju_logo.png" alt="WSFG" >
                </div>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Master Student</b> | Nanjing University<br>
                 Sep 2020 - June 2023. Advisor: Prof.<a href="https://scholar.google.com/citations?user=5kXEo74AAAAJ&hl=zh-CN">Chunlin Chen</a> and Prof. <a href="https://scholar.google.com/citations?user=AC-EDw0AAAAJ&hl=zh-CN">Huaxiong Li</a>
                  </p>
                </div>
              </div>
            </article>


	<article class="columns">
              <div class="column is-3" >
                <div class="image">
                  <img src="./Files/seu_logo.png" alt="WSFG" >
                </div>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Undergraduate Student</b> | South East University<br>
                 Sep 2016 - June 2020. 
                </div>
              </div>
            </article>


            <!--Selected Publications-->
            <h2 id="publications">
              Selected Publications
              <span style="font-size: 1rem;margin-left: 1rem;position: relative;bottom: .2rem;">
		<a href="https://chenhaoxing.github.io/publication.html" target="_blank" style="font-size: 20px;">
                  [Full List]
                </a>
                <a href="https://scholar.google.co.uk/citations?user=BnS7HzAAAAAJ&hl=en" target="_blank" style="font-size: 20px;">
                  [Google Scholar]
                </a>
                <a href="https://dblp.org/pid/168/5619.html" target="_blank" style="font-size: 20px;">
                  [DBLP]
                </a>
              </span>
            </h2>

            <!--List of publications--> 

	<article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/shuffle_mamba.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Stochastic Layer-Wise Shuffle: A Good Practice to Improve Vision Mamba Training</b><br>
                    Zizheng Huang, <b>Haoxing Chen</b>,Jiaqi Li, Jun Lan, Huijia Zhu, Weiqiang Wang, Limin Wang.<br>
                    In: <i>International Conference on Machine Learning (<b>ICML</b>)</i>, 2025. (<b>CCF-A</b>) <br>
		<button class="button button4"><a href="https://arxiv.org/pdf/2408.17081.pdf" target="_blank">Paper</a></button>  <button class="button button4"><a href="https://github.com/huangzizheng01/ShuffleMamba" target="_blank">Code</a></button>
                  </p>
                </div>
              </div>
            </article>
		  
		  
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/MSTA.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Efficient Transfer Learning for Video-language Foundation Models</b><br>
                    <b>Haoxing Chen</b>, Zizheng Huang, Yan Hong, Yanshuo Wang, Zhongcai Lyu, Zhuoer Xu, Jun Lan, Zhangxuan Gu.
                    In: <i>IEEE Conference on Computer Vision and Pattern Recognition(<b>CVPR</b>)</i>, 2025. (<b>CCF-A</b>) <br>
		[<a href="https://arxiv.org/pdf/2411.11223.pdf" target="_blank">Paper</a>]  
                  </p>
                </div>
              </div>
            </article>

		  
	<article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/wildfake.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection</b><br>
                    Yan Hong, Jianming Feng, <b>Haoxing Chen</b>, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang.<br>
                    In: <i>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</i>, 2025.(<b>CCF-A</b>) <strong><font color=#e74d3c>Oral</font></strong>  <br>
                  [<a href="https://modelscope.cn/datasets/hy2628982280/WildFake/summary" target="_blank">Data</a>]  
		  </p>
                </div>
              </div>
            </article>
		  
	<article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/degree_pr_2024.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Learning Latent Distangled Embeddings and Graphs for Multi-view Clustering</b><br>
                    Chao Zhang, <b>Haoxing Chen</b>, Huaxiong Li, Chunlin Chen.<br>
                    <i>Pattern Recognit., 2024. (<b>CCF-B, SCI/SCIE, Impact Factor: 7.5</b>)</i> <br>
		[<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320324005909" target="_blank">Paper</a>]  
                  </p>
                </div>
              </div>
            </article>
		  
	<article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/ComFusion.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>ComFusion: Personalized Subject Generation in Multiple Specific Scenes From Single Image</b><br>
                    Yan Hong, Yuxuan Duan, Bo Zhang, <b>Haoxing Chen</b>, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang.<br>
                    In: <i>European Conference on Computer Vision (<b>ECCV</b>)</i>, 2024.(<b>CCF-B</b>)
                  </p>
                </div>
              </div>
            </article>
		  
          <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/srin.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Segment Anything Model Meets Image Harmonization</b><br>
                    <b>Haoxing Chen</b>, Yaohui Li,, Zhangxuan Gu,  Zhuoer Xu, Jun Lan, Huaxiong Li.<br>
                    In: <i>IEEE International Conference on Acoustics, Speech and Signal Processing(<b>ICASSP</b>)</i>, 2024. (<b>CCF-B</b>)<br>
                    [<a href="./Files/SRIN_ICASSP24.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2312.12729" target="_blank">arXiv</a>]
                    [<a href="https://dblp.org/rec/journals/corr/abs-2312-12729.html?view=bibtex" arget="_blank">BibTex</a>]
                  </p>
                </div>
              </div>
            </article>

          <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/arch.jpeg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>DiffusionInst: Diffusion Model for Instance Segmentation</b><br>
                    Zhangxuan Gu, <b>Haoxing Chen</b>, Zhuoer Xu, Jun Lan, Changhua Meng, Weiqiang Wang.<br>
                    In: <i>IEEE International Conference on Acoustics, Speech and Signal Processing(<b>ICASSP</b>)</i>, 2024. (<b>CCF-B</b>) <strong><font color=#e74d3c>Oral</font></strong>  <br>
                    [<a href="./Files/Diffusioninst.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2212.02773" target="_blank">arXiv</a>]
                    [<a href="https://github.com/chenhaoxing/DiffusionInst" target="_blank">Code</a>]
                    [<a href="https://github.com/ant-research/diffusion-model-for-instance-segmentation" target="_blank">Code(Ant-Research)</a>]
                    [<a href="https://dblp.org/rec/journals/corr/abs-2212-02773.html?view=bibtex" arget="_blank">BibTex</a>]    
		    <a href="https://github.com/chenhaoxing/DiffusionInst" target="_blank"><strong><font color=#e74d3c>200+ GitHub Stars</font></strong></a>
                  </p>
                </div>
              </div>
            </article>
            
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/DiffUTE.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>DiffUTE: Universal Text Editing Diffusion Model</b><br>
                    <b>Haoxing Chen</b>, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Xing Zheng, Yaohui Li, Changhua Meng, Huijia Zhu, Weiqiang Wang.<br>
                    In: <i>Thirty-seventh Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</i>, 2023. (<b>CCF-A</b>)<br>
                    [<a href="./Files/DiffUTE.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2305.10825" target="_blank">arXiv</a>]
                    [<a href=" https://github.com/chenhaoxing/DiffUTE" target="_blank">Code</a>]
                    [<a href=" https://www.bilibili.com/video/BV1wT4y1h7c7/" target="_blank">Video</a>]
		    <a href="https://github.com/chenhaoxing/DiffUTE" target="_blank"><strong><font color=#e74d3c>100+ GitHub Stars</font></strong></a>
                  </p>
                </div>
              </div>
            </article>

          <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/hd.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Hierarchical Dynamic Image Harmonization</b><br>
                    <b>Haoxing Chen</b>, Zhangxuan Gu, Yaohui Li, Jun Lan, Changhua Meng, Weiqiang Wang, Huaxiong Li.<br>
                    In: <i>ACM Multimedia (<b>ACM MM</b>)</i>, 2023. (<b>CCF-A</b>) <strong><font color=#e74d3c>Oral</font></strong> <br>
                    [<a href="./Files/HDNet.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2211.08639" target="_blank">arXiv</a>]
                    [<a href="https://github.com/chenhaoxing/HDNet" target="_blank">Code</a>]
                    [<a href="https://dblp.org/rec/journals/corr/abs-2211-08639.html?view=bibtex" arget="_blank">BibTex</a>]
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/macl.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Model-Aware Contrastive Learning: Towards Escaping the Dilemmas</b><br>
                    Zizheng Huang<sup>#</sup>, <b>Haoxing Chen <sup>#</sup>* </b>, Ziqi Wen, Chao Zhang, Huaxiong Li, Bo Wang, Chunlin Chen. 
                    In: <i>International Conference on Machine Learning(<b>ICML</b>)</i>, 2023. (<b>CCF-A</b>) [# Equal contribution, * Corresponding author] <br>
                    [<a href="./Files/macl.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2207.07874" target="_blank">arXiv</a>]
                    [<a href="https://github.com/chenhaoxing/MACL_ICML2023" target="_blank">Code</a>]
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/ss.png" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Sparse Spatial Transformers for Few-Shot Learning</b><br>
                    <b>Haoxing Chen</b>, Huaxiong Li, Yaohui Li, Chunlin Chen.
                    <i>Sci. China Inf. Sci., 2023, 66(11): 210102. (<b>CCF-A, SCI/SCIE, Impact Factor: 8.8</b>)</i> <br>
                    [<a href="./Files/chx-ssformers.pdf" target="_blank">Paper</a>]
                    [<a href="http://engine.scichina.com/doi/10.1007/s11432-022-3700-8" target="_blank">SCIS Link</a>]
                     [<a href=" https://github.com/chenhaoxing/ssformers" target="_blank">Code</a>]
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/apt2023.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Mobile User Interface Element Detection Via Adaptively Prompt Tuning</b><br>
                    Zhangxuan Gu, Zhuoer Xu, <b>Haoxing Chen</b>, Jun Lan, Changhua Meng, Weiqiang Wang.
                    In: <i>IEEE Conference on Computer Vision and Pattern Recognition(<b>CVPR</b>)</i>, 2023. (<b>CCF-A</b>) <br>
                    [<a href="./Files/CVPR23.pdf" target="_blank">Paper</a>]
                    [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf" target="_blank">CVPR Link</a>]
                     [<a href="https://github.com/antmachineintelligence/MUI-zh" target="_blank">Code</a>]
                    [<a href="https://dblp.org/rec/conf/cvpr/GuXCLMW23.html?view=bibtex" arget="_blank">BibTex</a>]
                  </p>
                </div>
              </div>
            </article>

		<article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/mm_tapp.png" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Transductive Aesthetic Preference Propagation for Personalized Image Aesthetics Assessment</b><br>
                    Yaohui Li, Yuzhe Yang, Huaxiong Li, <b>Haoxing Chen</b>, Liwu Xu, Leida Li, Yaqian Li, Yandong Guo.
                    In: <i>ACM Multimedia (<b>ACM MM</b>)</i>, 2022. (<b>CCF-A</b>)<br>
                    [<a href="./Files/TAPP-PIAA.pdf" target="_blank">Paper</a>] [<a href="https://dblp.org/rec/conf/mm/LiYLCXLLG22.html?view=bibtex" target="_blank">BibTex</a>]
                  </p>
                </div>
              </div>
            </article>
		              
            <!--Awards-->
            <h2 id="awards">Awards</h2>
            <ul>
                <li>2023, 2nd place (2/717) in the tamper-proof financial documents track in the <a href="https://tianchi.aliyun.com/competition/entrance/532096/information">AFAC Financial Data Verification Competition</a>.</li>
                <li>2023, Nanjing University (NJU) Outstanding Graduates.</li>
                <li>2023, 3rd place (3/1267) in the classification track and 6th place (6/1156) in the detection track in the <a href="https://tianchi.aliyun.com/competition/entrance/532048/introduction?spm=5176.12281976.0.0.4562331137uPvV">ICDAR Detecting Tampered Text in Images Competition</a>.</li>  
                <li>2022, Chinese National Scholarship.</li>
                <li>2019, Meritorious Prize in the Mathematical Contest In Modeling (MCM).</li>
                <li>2018, First Prize of Jiangsu Province in the National Mathematical Modelling Competition.</li>
                <li>2018, National Special Award of the 8th Education Robot Competition Of China (ERCC).</li>
            </ul>

            <h2 id="services">Services</h2>
            <ul>
            <li>ICLR'25, ICCV'25, CVPR'25, ICME'25, ICML'24/25, NeurIPS'24, WACV'24, ACM MM'23/24, AAAI'23/25, PAKDD'22, ICPR'22, Reviewer</li>
            <li>IEEE Trans on TIP/TCYB/TMM/TNNLS/TCSVT, Reviewer</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
</section>

  <script>
    var $hashList = $('.menu-list a'), offsetList, maxScrollHeight;

    $('#sidebar').on('click', 'a', function(){
      activate($(this))
    });

    $(window).on('resize', debounce(calculateBoundary, 300));

    $(document).on('scroll', debounce(judgeScroll, 300));

    calculateBoundary();
    judgeScroll();
    
    function  calculateBoundary() {
      offsetList = $hashList.map(function(idx, ele){
        return $(ele.hash).offset().top
      });
      maxScrollHeight = $(document).height() - $(window).height()
    }

    function judgeScroll() {
      var tps = $("html").scrollTop()
              ? $("html").scrollTop()
              : $("body").scrollTop(),
              len = offsetList.length;
      if(tps >= maxScrollHeight-10){
        activate($hashList.eq(len-1));
        return
      }
      for(var i=0; i<len; i++){
       if(tps+50<offsetList[i]){
          activate($hashList.eq(Math.max(0,i-1)));
          return
        }
      }
    }

    function activate(ele){
      $hashList.removeClass('is-active');
      ele.addClass('is-active');
    }

    function debounce(fn, delay) {
      var timeout = null;
      return function () {
        var args = arguments;
        var context = this;
        if (!timeout) {
          timeout = setTimeout(function () {
            timeout = 0;
            return fn.apply(context, args);
          }, delay);
        }
      };
    }
  </script>
</body>

</html>
