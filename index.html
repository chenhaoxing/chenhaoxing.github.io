<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Haoxing Chen's Homepage</title>
  <link href="css/bootstrap.css" rel='stylesheet' type='text/css' />
  <link rel="shortcut icon" type="image/x-icon" href="./Files/mambaicon.png">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <!-- Bulma Version 0.7.5-->
  <link rel="stylesheet" href="https://unpkg.com/bulma@0.7.5/css/bulma.min.css" />
  <link href='css/style.css?_t=20200916' rel='stylesheet' type='text/css'>
  <script defer src="font-awesome/js/brands.min.js"></script>
  <script defer src="font-awesome/js/regular.min.js"></script>
  <script defer src="font-awesome/js/fontawesome.min.js"></script>
  <style>
    .blue {
      color: rgb(238, 77, 44);
      font-style: normal.
    }

    #intro {
      margin-top: 0em !important;
    }

    .content h3 {
      margin-bottom: 1em!important;
      margin-top: 2em!important;
    }

    .columns:not(:last-child) {
      margin-bottom: 1.75rem!important;
    }

    #sidebar {
      width: 75%;
    }

    .center-content {
      display: flex;
      align-items: center;
    }
	  
    @media screen and (min-width: 769px),
    print {
      .column.is-3_10,
      .column.is-2-tablet {
        flex: none;
        width: 17%;
      }
    }
  </style>
  <script src="https://code.jquery.com/jquery-3.1.1.min.js" crossorigin="anonymous"></script>
</head>

<body>
  <section class="section">
    <div class="container">
      <div class="columns">
        <div class="column is-3_10">
          <div class="sticky">
             <figure class="image small-image" style="margin-top: 6px;">
              <img src="./Files/chx.jpg">
            </figure>
		  
            <div class="content">
              <h2 style="margin-top: 1em">Haoxing Chen</h2>
              <p>
                <b>AntGroup</b><br/> 
                Hangzhou, CHINA<br/> 
              </p>
		    <div class="slogan" style="font-style: italic; text-align: center; color: #4a4a4a;">
  "I'd rather go 0-30 than 0-9, because you learn nothing when you miss your first shot. When you go 0-9, you think you've failed yourself." â€” Kobe Bryant
</div>
            </div>
            <!-- social network icons -->
            <div class="social">
              <a href="https://github.com/chenhaoxing" target="_blank">
                <span class="fab fa-github fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a;"></span>
              </a>
              <a href="https://scholar.google.com/citations?hl=zh-CN&pli=1&user=BnS7HzAAAAAJ" target="_blank">
                <span class="fab fa-google fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a; margin-left: 15px"></span>
              </a>
              <a href="mailto:hx.chen@hotmail.com" target="_blank">
                <span class="fa-regular fa-envelope-open fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a; margin-left: 15px"></span>
              </a>
            </div>
            <!-- slogon -->
            <div id="sidebar" class="menu sticky is-hidden-mobile">
              <p class="menu-label"><b>Quick Links</b></p>
              <ul class="menu-list">
                <li><a href="#intro">About Me</a></li>
                <li><a href="#interest">Research Interest</a></li>
		<li><a href="#experiences">Experiences</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#awards">Awards</a></li>
                <li><a href="#services">Services</a></li>
              </ul>
            </div>
	 </div>
        </div>

        <div class="column right-panel">
          <div class="content">
            <!--About Me-->
            <h2 id="intro">About Me</h2>
            <p>
               I am an Artificial Intelligence Researcher with <a href="https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz=MzI2ODA2MjAwNw==&scene=123#wechat_redirect">Tiansuan Lab</a> at Ant Group, which is under the leadership of Dr. <a href="https://scholar.google.com/citations?user=yZ5iffAAAAAJ&hl=zh-CN">Weiqiang Wang</a>. My research benefits from collaboration with esteemed colleagues including Dr. <a href="https://scholar.google.com/citations?user=pC2kmQoAAAAJ&hl=zh-CN">Yaohui Li</a>, Dr. <a href="https://scholar.google.com/citations?user=Wkp3s68AAAAJ&hl=zh-CN">Zhangxuan Gu</a>, Dr. <a href="https://scholar.google.com/citations?user=ztq5-xcAAAAJ&hl=zh-CN">Yan Hong</a>, and Scientist <a href="https://scholar.google.com/citations?user=na24qQoAAAAJ&hl=zh-CN">Zhuoer Xu</a>.
            </p>

            <p>
             I pursued my M.E. in Automation and Artificial Intelligence Group at Nanjing University (NJU), where I was mentored by Prof. <a href="https://scholar.google.com/citations?user=5kXEo74AAAAJ&hl=zh-CN">Chunlin Chen</a> and Prof. <a href="https://scholar.google.com/citations?user=AC-EDw0AAAAJ&hl=zh-CN">Huaxiong Li</a>. My B.E. was obtained at Southeast University (SEU). 
          My research interests include few-shot learning, image generation, self-supervised learning, computer vision, and machine learning. Currently, I focus on Representation Learning, AIGC, and Learning with Limited Data.
             </p>

	<!--Research Interest-->
            <h2 id="interest" style="margin-bottom: 25px;">Research Interest</h2>
	I work in the field of few-shot learning, image generation, self-supervised learning, computer vision and machine learning. Currently, I focus on the following research topics:
	<ul>
	<li><strong>Representation Learning:</strong> Representation learning aims to discover abstract descriptions of concepts. Specifically, Haoxing focuses on enhancing the universality of the model through self-supervised learning and multimodal learning.</li>
	<li><strong>AIGC:</strong> How to design better defense systems to deal with generated attacks has gained extensive attention in recent years. Specifically, Haoxing trys to generate more realistic images and design better detection methods with multi-modal learning.</li>
	<li><strong>Learning with Limited Data:</strong> The ability of a model to fit with limited data is essential and necessary due to the instance/label collection cost. How to extract and utilize knowledge from related tasks and domains is the key. Specifically, Haoxing mainly works on how to learn meta-knowledge for zero-/few-shot learning.</li>
	</ul>
	<br />
		  
            <!--Experience-->
            <h2 id="experiences" style="margin-bottom: 25px;">Experiences</h2>
            
            <article class="columns">
              <div class="column is-3" >
                <div class="image">
                  <img src="./Files/ant.png" alt="WSFG">
                </div>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>AI Researcher</b> | Ant Group<br>
                 May 2022 - Present.</a>
                  </p>
                </div>
              </div>
	</article>

	<article class="columns">
              <div class="column is-3" >
                <div class="image">
                  <img src="./Files/nju_logo.png" alt="WSFG" >
                </div>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Master Student</b> | Nanjing University<br>
                 Sep 2020 - June 2023. Advisor: Prof.<a href="https://scholar.google.com/citations?user=5kXEo74AAAAJ&hl=zh-CN">Chunlin Chen</a> and Prof. <a href="https://scholar.google.com/citations?user=AC-EDw0AAAAJ&hl=zh-CN">Huaxiong Li</a>
                  </p>
                </div>
              </div>
            </article>


	<article class="columns">
              <div class="column is-3" >
                <div class="image">
                  <img src="./Files/seu_logo.png" alt="WSFG" >
                </div>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Undergraduate Student</b> | South East University<br>
                 Sep 2016 - June 2020. 
                </div>
              </div>
            </article>


            <!--Selected Publications-->
            <h2 id="publications">
              Selected Publications
              <span style="font-size: 1rem;margin-left: 1rem;position: relative;bottom: .2rem;">
		<a href="https://chenhaoxing.github.io/publication.html" target="_blank" style="font-size: 20px;">
                  [Full List]
                </a>
                <a href="https://scholar.google.com/citations?hl=zh-CN&pli=1&user=BnS7HzAAAAAJ" target="_blank" style="font-size: 20px;">
                  [Google Scholar]
                </a>
                <a href="https://dblp.org/pid/168/5619.html" target="_blank" style="font-size: 20px;">
                  [DBLP]
                </a>
              </span>
            </h2>

            <!--List of publications--> 

	<article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/ComFusion.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>ComFusion: Personalized Subject Generation in Multiple Specific Scenes From Single Image</b><br>
                    Yan Hong, Yuxuan Duan, Bo Zhang, <b>Haoxing Chen</b>, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang.<br>
                    <i>European Conference on Computer Vision (<b>ECCV</b>)</i>, 2024.(<b>CCF-B</b>)
                  </p>
                </div>
              </div>
            </article>
		  
          <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/srin.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Segment Anything Model Meets Image Harmonization</b><br>
                    <b>Haoxing Chen</b>, Yaohui Li,, Zhangxuan Gu,  Zhuoer Xu, Jun Lan, Huaxiong Li.<br>
                    In: <i>IEEE International Conference on Acoustics, Speech and Signal Processing(<b>ICASSP</b>)</i>, 2024. (<b>CCF-B</b>)<br>
                    [<a href="./Files/SRIN_ICASSP24.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2312.12729" target="_blank">arXiv</a>]
                    [<a href="https://dblp.org/rec/journals/corr/abs-2312-12729.html?view=bibtex" arget="_blank">BibTex</a>]
                  </p>
                </div>
              </div>
            </article>

          <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/arch.jpeg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>DiffusionInst: Diffusion Model for Instance Segmentation</b><br>
                    Zhangxuan Gu, <b>Haoxing Chen</b>, Zhuoer Xu, Jun Lan, Changhua Meng, Weiqiang Wang.<br>
                    In: <i>IEEE International Conference on Acoustics, Speech and Signal Processing(<b>ICASSP</b>)</i>, 2024. (<b>CCF-B</b>) <strong><font color=#e74d3c>Oral</font></strong>  <br>
                    [<a href="./Files/Diffusioninst.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2212.02773" target="_blank">arXiv</a>]
                    [<a href="https://github.com/chenhaoxing/DiffusionInst" target="_blank">Code</a>]
                    [<a href="https://github.com/ant-research/diffusion-model-for-instance-segmentation" target="_blank">Code(Ant-Research)</a>]
                    [<a href="https://dblp.org/rec/journals/corr/abs-2212-02773.html?view=bibtex" arget="_blank">BibTex</a>]    
		    <a href="https://github.com/chenhaoxing/DiffusionInst" target="_blank"><strong><font color=#e74d3c>200+ GitHub Stars</font></strong></a>
                  </p>
                </div>
              </div>
            </article>
            
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/DiffUTE.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>DiffUTE: Universal Text Editing Diffusion Model</b><br>
                    <b>Haoxing Chen</b>, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Xing Zheng, Yaohui Li, Changhua Meng, Huijia Zhu, Weiqiang Wang.<br>
                    In: <i>Thirty-seventh Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</i>, 2023. (<b>CCF-A</b>)<br>
                    [<a href="./Files/DiffUTE.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2305.10825" target="_blank">arXiv</a>]
                    [<a href=" https://github.com/chenhaoxing/DiffUTE" target="_blank">Code</a>]
                    [<a href=" https://www.bilibili.com/video/BV1wT4y1h7c7/" target="_blank">Video</a>]
		    <a href="https://github.com/chenhaoxing/DiffUTE" target="_blank"><strong><font color=#e74d3c>100+ GitHub Stars</font></strong></a>
                  </p>
                </div>
              </div>
            </article>

          <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/hd.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Hierarchical Dynamic Image Harmonization</b><br>
                    <b>Haoxing Chen</b>, Zhangxuan Gu, Yaohui Li, Jun Lan, Changhua Meng, Weiqiang Wang, Huaxiong Li.<br>
                    In: <i>ACM Multimedia (<b>ACM MM</b>)</i>, 2023. (<b>CCF-A</b>) <strong><font color=#e74d3c>Oral</font></strong> <br>
                    [<a href="./Files/HDNet.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2211.08639" target="_blank">arXiv</a>]
                    [<a href="https://github.com/chenhaoxing/HDNet" target="_blank">Code</a>]
                    [<a href="https://dblp.org/rec/journals/corr/abs-2211-08639.html?view=bibtex" arget="_blank">BibTex</a>]
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/macl.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Model-Aware Contrastive Learning: Towards Escaping the Dilemmas</b><br>
                    Zizheng Huang<sup>#</sup>, <b>Haoxing Chen <sup>#</sup>* </b>, Ziqi Wen, Chao Zhang, Huaxiong Li, Bo Wang, Chunlin Chen. 
                    In: <i>International Conference on Machine Learning(<b>ICML</b>)</i>, 2023. (<b>CCF-A</b>) [# Equal contribution, * Corresponding author] <br>
                    [<a href="./Files/macl.pdf" target="_blank">Paper</a>]
                    [<a href="https://arxiv.org/abs/2207.07874" target="_blank">arXiv</a>]
                    [<a href="https://github.com/chenhaoxing/MACL_ICML2023" target="_blank">Code</a>]
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/ss.png" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Sparse Spatial Transformers for Few-Shot Learning</b><br>
                    <b>Haoxing Chen</b>, Huaxiong Li, Yaohui Li, Chunlin Chen.
                    <i>Sci. China Inf. Sci., 2023, 66(11): 210102. (<b>CCF-A, SCI/SCIE, Impact Factor: 8.8</b>)</i> <br>
                    [<a href="./Files/chx-ssformers.pdf" target="_blank">Paper</a>]
                    [<a href="http://engine.scichina.com/doi/10.1007/s11432-022-3700-8" target="_blank">SCIS Link</a>]
                     [<a href=" https://github.com/chenhaoxing/ssformers" target="_blank">Code</a>]
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/apt2023.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Mobile User Interface Element Detection Via Adaptively Prompt Tuning</b><br>
                    Zhangxuan Gu, Zhuoer Xu, <b>Haoxing Chen</b>, Jun Lan, Changhua Meng, Weiqiang Wang.
                    In: <i>IEEE Conference on Computer Vision and Pattern Recognition(<b>CVPR</b>)</i>, 2023. (<b>CCF-A</b>) <br>
                    [<a href="./Files/CVPR23.pdf" target="_blank">Paper</a>]
                    [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf" target="_blank">CVPR Link</a>]
                     [<a href="https://github.com/antmachineintelligence/MUI-zh" target="_blank">Code</a>]
                    [<a href="https://dblp.org/rec/conf/cvpr/GuXCLMW23.html?view=bibtex" arget="_blank">BibTex</a>]
                  </p>
                </div>
              </div>
            </article>

		<article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="./Files/apt2023.jpg" alt="WSFG" style="box-shadow: 4px 4px 8px #888;">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Transductive Aesthetic Preference Propagation for Personalized Image Aesthetics Assessment</b><br>
                    Yaohui Li, Yuzhe Yang, Huaxiong Li, <b>Haoxing Chen</b>, Liwu Xu, Leida Li, Yaqian Li, Yandong Guo.
                    In: <i>ACM Multimedia (<b>ACM MM</b>)</i>, 2022. (<b>CCF-A</b>)<br>
                    [<a href="./Files/TAPP-PIAA.pdf" target="_blank">Paper</a>] [<a href="https://dblp.org/rec/conf/mm/LiYLCXLLG22.html?view=bibtex" target="_blank">BibTex</a>]
                  </p>
                </div>
              </div>
            </article>
		              
            <!--Awards-->
            <h2 id="awards">Awards</h2>
            <ul>
                <li>2023, 2nd place (2/717) in the tamper-proof financial documents track in the <a href="https://tianchi.aliyun.com/competition/entrance/532096/information">AFAC Financial Data Verification Competition</a>.</li>
                <li>2023, Nanjing University (NJU) Outstanding Graduates.</li>
                <li>2023, 3rd place (3/1267) in the classification track and 6th place (6/1156) in the detection track in the <a href="https://tianchi.aliyun.com/competition/entrance/532048/introduction?spm=5176.12281976.0.0.4562331137uPvV">ICDAR Detecting Tampered Text in Images Competition</a>.</li>  
                <li>2022, Chinese National Scholarship.</li>
                <li>2019, Meritorious Prize in the Mathematical Contest In Modeling (MCM).</li>
                <li>2018, First Prize of Jiangsu Province in the National Mathematical Modelling Competition.</li>
                <li>2018, National Special Award of the 8th Education Robot Competition Of China (ERCC).</li>
            </ul>

            <h2 id="services">Services</h2>
            <ul>
            <li>NeurIPS'24, WACV'24, ACM MM'23/24, AAAI'23, PAKDD'22, ICPR'22, Reviewer</li>
            <li>IEEE Trans on TIP/TCYB/TMM/TNNLS/TCSVT, Reviewer</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
</section>

  <script>
    var $hashList = $('.menu-list a'), offsetList, maxScrollHeight;

    $('#sidebar').on('click', 'a', function(){
      activate($(this))
    });

    $(window).on('resize', debounce(calculateBoundary, 300));

    $(document).on('scroll', debounce(judgeScroll, 300));

    calculateBoundary();
    judgeScroll();
    
    function  calculateBoundary() {
      offsetList = $hashList.map(function(idx, ele){
        return $(ele.hash).offset().top
      });
      maxScrollHeight = $(document).height() - $(window).height()
    }

    function judgeScroll() {
      var tps = $("html").scrollTop()
              ? $("html").scrollTop()
              : $("body").scrollTop(),
              len = offsetList.length;
      if(tps >= maxScrollHeight-10){
        activate($hashList.eq(len-1));
        return
      }
      for(var i=0; i<len; i++){
       if(tps+50<offsetList[i]){
          activate($hashList.eq(Math.max(0,i-1)));
          return
        }
      }
    }

    function activate(ele){
      $hashList.removeClass('is-active');
      ele.addClass('is-active');
    }

    function debounce(fn, delay) {
      var timeout = null;
      return function () {
        var args = arguments;
        var context = this;
        if (!timeout) {
          timeout = setTimeout(function () {
            timeout = 0;
            return fn.apply(context, args);
          }, delay);
        }
      };
    }
  </script>
</body>

</html>
