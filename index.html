<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Haoxing Chen, 陈昊星, few-shot learning, metric-learning, self-supervised learning, computer vision, machine learning, Nanjing University">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />	
<script src="jquery.min.js"></script>
<title>Haoxing Chen (陈昊星)⭐ </title> 
    
    <head>
        <style type="text/css">
            body{margin: 20px 140px;}
		h2{color:rgb(24,144,255);}
		a{text-decoration:underline}
		a:link {
			color:#e7767f;
			text-decoration:none;
		}
		:visited {
			color:grey;
			text-decoration:none;
		}
		a:hover {
			color:orange;
			text-decoration:none;
		}
		
		a:active {
			color:black;
			text-decoration:none;
		}
		*{ 
 padding: 0px; margin: 0px; } .nav{ 
 list-style: none; /*background-color:rgb(24,144,255);*//*给整个列表设置蓝色背景*/ /*width: 1200px;*/ /*height: 45px;*/ margin: 20px auto; overflow: hidden; zoom: 1; } .nav li{ 
 float: left; width: 13%; } .nav a{ 
 width: 100%; display: inline-block; text-align: center; padding: 5px 0px; text-decoration: none; color: Black; font-weight: bold} .nav a:hover{ 
 background-color:rgb(24,144,255);color: White; } 
        </style>
	<link rel="icon" href="./Files/mambaicon.png">
    </head>

	
<body>
<ul class="nav">
<li><a href="/#-Bio"><font size="4">Biography</font></a></li>
<li><a href="#-News"><font size="4">News</font></a></li>
<li><a href="#-Pre"><font size="4">Preprints</font></a></li>
<li><a href="/#-Pub"><font size="4">Publications</font></a></li>
<li><a href="#-Awards"><font size="4">Awards</font></a></li>
<li><a href="/#-Service"><font size="4">Services</font></a></li>
</ul>
		
    <div id="layout-content">
       <table class="imgtable">
            <tbody>
                <tr> 
                    
                    <td>
                        <a href="./">
                            <img src="./Files/chx.jpg" width="240px" height="300px">
                        </a>
                        &nbsp;
                   
                    
                    <td align="left">
                        <p>
                            <b>
                                <font size="+3">Haoxing Chen</font>
                                <font size="+3" face="华文行楷">(陈昊星) ⭐</font>
                                </b>
                            <br>
				<p>
       We are looking for foundation model/video understanding research interns, feel free to contact us.
        </p>
        <p>
  I am an Artificial Intelligence Researcher with <a href="https://mp.weixin.qq.com/mp/profile_ext?action=home&__biz=MzI2ODA2MjAwNw==&scene=123#wechat_redirect">Tiansuan Lab</a> at Ant Group, which is under the leadership of Dr. <a href="https://scholar.google.com/citations?user=yZ5iffAAAAAJ&hl=zh-CN">Weiqiang Wang</a>. My research benefits from collaboration with esteemed colleagues including Dr. <a href="https://scholar.google.com/citations?user=pC2kmQoAAAAJ&hl=zh-CN">Yaohui Li</a>, Dr. <a href="https://scholar.google.com/citations?user=Wkp3s68AAAAJ&hl=zh-CN">Zhangxuan Gu</a>, Dr. <a href="https://scholar.google.com/citations?user=ztq5-xcAAAAJ&hl=zh-CN">Yan Hong</a>, and Scientist <a href="https://scholar.google.com/citations?user=na24qQoAAAAJ&hl=zh-CN">Zhuoer Xu</a>.
</p>
<p>
  I pursued my M.E. in Automation and Artificial Intelligence Group at Nanjing University (NJU), where I was mentored by Prof. <a href="https://scholar.google.com/citations?user=5kXEo74AAAAJ&hl=zh-CN">Chunlin Chen</a> and Prof. <a href="https://scholar.google.com/citations?user=AC-EDw0AAAAJ&hl=zh-CN">Huaxiong Li</a>. My B.E. was obtained at Southeast University (SEU). 
          My research interests include few-shot learning, image generation, self-supervised learning, computer vision, and machine learning. Currently, I focus on Representation Learning, AIGC, and Learning with Limited Data.
        </p>
                          <p style="text-align:left">
          <a href="mailto:hx.chen@hotmail.com">Email</a> &nbsp;/&nbsp;
          <a href="https://scholar.google.com/citations?hl=zh-CN&pli=1&user=BnS7HzAAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
          <a href="https://github.com/chenhaoxing">Github</a>
        </p>
                        </td>
                          <td valign="top" width="58">
                    </td>
                </tr>
            </tbody>
        </table>

       
<td align="bottom" colspan="2" style="margin-top:2px">
	
<h2 id="-News">News !</h2>
<div style="height: 250px; overflow: auto;">
<ul>
<li><strong>[Dec./2023]: </strong>Two paper on “Diffusion model” and “Image composition” is accepted to "<a href="https://2024.ieeeicassp.org/">ICASSP 2024</a>".</li>
<li><strong>[Sep./2023]: </strong>One paper on “AIGC” is accepted to "<a href="https://nips.cc/">NeurIPS 2023</a>".</li>
<li><strong>[Aug/2023]: </strong>We won 2nd place (2/717) in the tamper-proof financial documents track in the <a href="https://tianchi.aliyun.com/competition/entrance/532096/information">AFAC Financial Data Verification Competition</a></li>
<li><strong>[July/2023]: </strong>One paper on “Image composition” is accepted to "<a href="https://www.acmmm2023.org/">ACM Multimedia 2023</a>" as Oral representation.</li>
<li><strong>[April/2023]: </strong>One paper on “Contrastive learning” is accepted to <a href="https://icml.cc/Conferences/2023/">ICML 2023</a>.</li>
<li><strong>[Mar./2023]: </strong>We won 3rd place (3/1267) in the classification track and 6th place (6/1156) in the detection track in the <a href="https://tianchi.aliyun.com/competition/entrance/532048/introduction?spm=5176.12281976.0.0.4562331137uPvV">ICDAR Detecting Tampered Text in Images Competition</a>.</li>
<li><strong>[Feb./2023]: </strong>One paper on “Vision-language learning” is accepted to <a href="https://cvpr.thecvf.com/">CVPR 2023</a>.</li>
<li><strong>[Jan./2023]: </strong>One paper on “Few-shot learning” is accepted to <a href="http://engine.scichina.com/doi/10.1007/s11432-022-3700-8">SCIS 2023</a>.</li>
<li><strong>[July/2022]: </strong>One paper on “Affective computing” is accepted to <a href="https://2022.acmmm.org/">ACM Multimedia 2022</a>.</li>
</ul>
</div>
</td>
<br />
	    
        <h2 id="-RI">Research Interest </h2>
I work in the field of few-shot learning, image generation, self-supervised learning, computer vision and machine learning. Currently, I focus on the following research topics:
<ul>
<li><strong>Representation Learning:</strong> Representation learning aims to discover abstract descriptions of concepts. Specifically, Haoxing focuses on enhancing the universality of the model through self-supervised learning and multimodal learning.</li>
<li><strong>AIGC:</strong> How to design better defense systems to deal with generated attacks has gained extensive attention in recent years. Specifically, Haoxing trys to generate more realistic images and design better detection methods with multi-modal learning.</li>
<li><strong>Learning with Limited Data:</strong> The ability of a model to fit with limited data is essential and necessary due to the instance/label collection cost. How to extract and utilize knowledge from related tasks and domains is the key. Specifically, Haoxing mainly works on how to learn meta-knowledge for zero-/few-shot learning.</li>
</ul>
<br />
	    
	    
        <h2 id="-Pre">Selected Publications [<a href="https://chenhaoxing.github.io/publication.html" target="_blank">Full List</a>]</h2>

	<table class="imgtable">
                <tbody>
                    <tr>
                        <td>
                            <img src="./Files/srin.jpg" alt="WSFG" width="220px" height="110px"  style="box-shadow: 4px 4px 8px #888">
                            &nbsp;
                        </td>
                        <td align="left">
                            <ul>
                                <li>
                                    <p>
                                        <b>Haoxing Chen</b>, Yaohui Li,
                                        , Zhangxuan Gu,  Zhuoer Xu, Jun Lan, Huaxiong Li.
					    <br>
                                        <a href="https://arxiv.org/abs/2312.12729">Segment Anything Model Meets Image Harmonization.
					</a>
                                        <br>
                                        In: <em>IEEE International Conference on Acoustics, Speech and Signal Processing(<b>ICASSP</b>)</em>
                                        , 2024. (<b>CCF-B</b>)
                                        <br>
					[<a href="./Files/SRIN_ICASSP24.pdf" target="_blank">Paper</a>]
					 [<a href="https://dblp.org/rec/journals/corr/abs-2312-12729.html?view=bibtex" arget="_blank">BibTex</a>]
                                        </p>
                                </li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>


	       
	 <table class="imgtable">
                <tbody>
                    <tr>
                        <td>
                            <img src="./Files/arch.jpeg" alt="WSFG" width="220px" height="110px"  style="box-shadow: 4px 4px 8px #888">
                            &nbsp;
                        </td>
                        <td align="left">
                            <ul>
                                <li>
                                    <p>
                                        Zhangxuan Gu, <b>Haoxing Chen</b>
                                        , Zhuoer Xu, Jun Lan, Changhua Meng, Weiqiang Wang.
                                        <br>
                                        <a href="https://arxiv.org/abs/2212.02773">DiffusionInst: Diffusion Model for Instance Segmentation.</a>
                                        <br>
                                        In: <em>IEEE International Conference on Acoustics, Speech and Signal Processing(<b>ICASSP</b>)</em>
                                        , 2024. (<b>CCF-B</b>)
                                        <br>
                                        [<a href="./Files/Diffusioninst.pdf" target="_blank">Paper</a>]
                                        [<a href="https://github.com/chenhaoxing/DiffusionInst" target="_blank">Code</a>]
				        [<a href="https://github.com/ant-research/diffusion-model-for-instance-segmentation" target="_blank">Code(Ant-Research)</a>]
					[<a href="https://dblp.org/rec/journals/corr/abs-2212-02773.html?view=bibtex" arget="_blank">BibTex</a>]    
					<a href="https://github.com/chenhaoxing/DiffusionInst" target="_blank"><strong><font color=#e74d3c>200+ GitHub Stars</font></strong></a>
                                    </p>
                                </li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>    

	       
		<table class="imgtable">
                <tbody>
                    <tr>
                        <td>
                            <img src="./Files/DiffUTE.jpg" alt="WSFG" width="220px" height="110px" style="box-shadow: 4px 4px 8px #888">
                            &nbsp;
                        </td>
                        <td align="left">
                            <ul>
                                <li>
                                    <p>
                                        <b>Haoxing Chen</b>, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Xing Zheng, Yaohui Li, Changhua Meng, Huijia Zhu, Weiqiang Wang.
                                        <br>
                                        <a href="https://arxiv.org/abs/2305.10825">DiffUTE: Universal Text Editing Diffusion Model.</a>
                                        <br>
                                        In: <em>Thirty-seventh Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>
                                        , 2023. (<b>CCF-A</b>)
					<br>
                                        [<a href="./Files/DiffUTE.pdf" target="_blank">Paper</a>]
                                        [<a href=" https://github.com/chenhaoxing/DiffUTE" target="_blank">Code</a>]
					[<a href=" https://www.bilibili.com/video/BV1wT4y1h7c7/" target="_blank">Video</a>]
                                    </p>
                                </li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>    

	       
 <table class="imgtable">
                <tbody>
                    <tr>
                        <td>
                            <img src="./Files/hd.jpg" alt="WSFG" width="220px" height="110px" style="box-shadow: 4px 4px 8px #888">
                            &nbsp;
                        </td>
                        <td align="left">
                            <ul>
                                <li>
                                    <p>
                                        <b>Haoxing Chen</b>
                                        , Zhangxuan Gu, Yaohui Li, Jun Lan, Changhua Meng, Weiqiang Wang, Huaxiong Li.
                                        <br>
                                        <a href="https://arxiv.org/abs/2211.08639">Hierarchical Dynamic Image Harmonization.</a>
                                        <br>
                                        In: <em>ACM Multimedia (<b>ACM MM</b>)</em>
                                        , 2023.[<b>Oral</b>] (<b>CCF-A</b>)
					    <br>
                                        [<a href="./Files/HDNet.pdf" target="_blank">Paper</a>]
                                        [<a href=" https://github.com/chenhaoxing/HDNet" target="_blank">Code</a>]
					[<a href="hhttps://dblp.org/rec/journals/corr/abs-2211-08639.html?view=bibtex" arget="_blank">BibTex</a>]
                                    </p>
                                </li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>    



	       <table class="imgtable">
                <tbody>
                    <tr>
                        <td>
                            <img src="./Files/macl.jpg" alt="WSFG" width="220px" height="110px" style="box-shadow: 4px 4px 8px #888">
                            &nbsp;
                        </td>
                        <td align="left">
                            <ul>
                                <li>
                                    <p>
                                        Zizheng Huang<sup>#</sup>, <b>Haoxing Chen <sup>#</sup>* </b>
                                        , Ziqi Wen, Chao Zhang, Huaxiong Li, Bo Wang, Chunlin Chen. 
                                        <br>
                                        <a href="https://arxiv.org/abs/2207.07874">Model-Aware Contrastive Learning: Towards Escaping the Dilemmas.</a>
                                        <br>
                                        In: <em>International Conference on Machine Learning(<b>ICML</b>)</em>, 2023.(<b>CCF-A</b>)[# Equal contribution, * Corresponding author]
					    <br>
                                        [<a href="./Files/macl.pdf" target="_blank">Paper</a>]
                                        [<a href="https://github.com/chenhaoxing/MACL_ICML2023" target="_blank">Code</a>]
                                    </p>
                                </li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table> 

	    	<table class="imgtable">
                <tbody>
                    <tr>
                        <td>
                            <img src="./Files/ss.png" alt="WSFG" width="220px" height="110px" style="box-shadow: 4px 4px 8px #888">
                            &nbsp;
                        </td>
                        <td align="left">
                            <ul>
                                <li>
                                    <p>
                                        <b>Haoxing Chen</b>
                                        , Huaxiong Li, Yaohui Li, Chunlin Chen.
                                        <br>
                                        <a href="http://engine.scichina.com/doi/10.1007/s11432-022-3700-8">Sparse Spatial Transformers for Few-Shot Learning.</a>
                                        <br>
                                        <em>Sci. China Inf. Sci.</em>
                                        , 2023, 66(11): 210102. (<b>CCF-A, SCI/SCIE, Impact Factor: 8.8</b>)
                                        <br>
                                        [<a href="./Files/chx-ssformers.pdf" target="_blank">Paper</a>]
                                        [<a href=" https://github.com/chenhaoxing/ssformers" target="_blank">Code</a>]
					[<a href="https://dblp.org/rec/journals/corr/abs-2109-12932.html?view=bibtex" arget="_blank">BibTex</a>]
                                    </p>
                                </li>
                            </ul>
                        </td>
                    </tr>
                </tbody>
            </table>
	       
        </ol>

     <br />

<h2 id="-Awards">Awards</h2>
<font size="3"> 
<ul>
<li>2023, 2nd place (2/717) in the tamper-proof financial documents track in the <a href="https://tianchi.aliyun.com/competition/entrance/532096/information">AFAC Financial Data Verification Competition</a>.</li>
<li>2023, Nanjing University (NJU) Outstanding Graduates.</li>
<li>2023, 3rd place (3/1267) in the classification track and 6th place (6/1156) in the detection track in the <a href="https://tianchi.aliyun.com/competition/entrance/532048/introduction?spm=5176.12281976.0.0.4562331137uPvV">ICDAR Detecting Tampered Text in Images Competition</a>.</li>	
<li>2022, Chinese National Scholarship.</li>
<li>2019, Meritorious Prize in the Mathematical Contest In Modeling (MCM).</li>
<li>2018, First Prize of Jiangsu Province in the National Mathematical Modelling Competition.</li>
<li>2018, National Special Award of the 8th Education Robot Competition Of China (ERCC).</li>
</ul>
</font>
<br />
        
<h2 id="-Service">Haoxing's Services</h2>
<font size="3"> 
<ul>
<li>ACM MM'23, AAAI'23, PAKDD'22, ICPR'22, Reviewer</li>
<li>IEEE Trans on TIP/TCYB/TMM/TNNLS/TCSVT, Reviewer</li>
</ul>
</font>
<br />


<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

</p>
    </tr>
  </tbody>
</table>
</ul>

   
</body>    
</html>

